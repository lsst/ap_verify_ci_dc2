#!/usr/bin/env python
# This file is part of ap_verify_ci_hits2015.
#
# Developed for the LSST Data Management System.
# This product includes software developed by the LSST Project
# (https://www.lsst.org).
# See the COPYRIGHT file at the top-level directory of this distribution
# for details of code ownership.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.

"""Script for simulating the APDB preloads generated by associating this
dataset's observations.

This script assumes visits are processed in ascending numerical order, with no
prior history. Prior sources must be provided through a different script.

This script requires that all other input datasets are already in preloaded/,
but does not require that export.yaml be up to date. It (or rather, the
pipeline configs) require that the repository be set up.

This script takes roughly 10 minutes to run on rubin-devl.
"""

import glob
import logging
import os
import subprocess
import sys
import tempfile

import lsst.log
from lsst.daf.butler import Butler, CollectionType, MissingCollectionError
import lsst.obs.base
import lsst.dax.apdb


logging.basicConfig(level=logging.INFO, stream=sys.stdout)
lsst.log.configure_pylog_MDC("DEBUG", MDC_class=None)


# Avoid explicit references to dataset package to maximize portability.
SCRIPT_DIR = os.path.abspath(os.path.dirname(__file__))
PIPE_DIR = os.path.join(SCRIPT_DIR, "..", "pipelines")
PRELOAD_TYPES = ["preloaded_*"]
DEST_DIR = os.path.join(SCRIPT_DIR, "..", "preloaded")
DEST_COLLECTION = "dia_catalogs"
DEST_RUN = DEST_COLLECTION + "/apdb"
SOURCE_REPO = "/repo/dc2"
SOURCE_COLLECTION = "u/abudlong/DM-46545/w_2024_46/DC2"


########################################
# Processing steps

def _clear_preloaded(butler):
    """Remove preloaded datasets from the collection's chain.

    If it exists, ``DEST_RUN`` is removed entirely to keep it from interfering
    with the rest of this script. Other runs are merely unlinked in case they
    would still be useful.

    Parameters
    ----------
    butler : `lsst.daf.butler.Butler`
        A writeable Butler pointing to this repository.
    """
    try:
        butler.collections.redefine_chain(DEST_COLLECTION, [])
    except MissingCollectionError:
        # No preloaded datasets to begin with
        return
    butler.removeRuns([DEST_RUN], unstore=True)


def _check_pipeline(butler):
    """Confirm that the pipeline is correctly configured.

    Parameters
    ----------
    butler : `lsst.daf.butler.Butler`
        A Butler pointing to this repository.
    """
    pipeline_file = os.path.join(PIPE_DIR, "ApPipe.yaml")
    pipeline = lsst.pipe.base.Pipeline.fromFile(pipeline_file)
    pipeline.addConfigOverride("parameters", "apdb_config", "foo")
    # Check that the configs load correctly; raises if there's a setup missing
    pipeline.to_graph()


def _transfer_catalogs(catalog_types, src_repo, run, dest_repo):
    """Copy preloaded catalogs between two repositories.

    Parameters
    ----------
    catalog_types : iterable [`str`]
        A query expression for dataset types for preloaded catalogs.
    src_repo : `lsst.daf.butler.Butler`
        The repository from which to copy the datasets.
    run : `str`
        The name of the run containing the catalogs in both ``src_repo``
        and ``dest_repo``.
    dest_repo : `lsst.daf.butler.Butler`
        The repository to which to copy the datasets.
    """
    expanded_types = src_repo.registry.queryDatasetTypes(catalog_types)
    datasets = set()
    for t in expanded_types:
        datasets.update(src_repo.query_datasets(t, collections=run, explain=False))
    dest_repo.transfer_from(src_repo, datasets, transfer="copy",
                            register_dataset_types=True, transfer_dimensions=True)


########################################
# Put everything together

preloaded = Butler(DEST_DIR, writeable=True)
_check_pipeline(preloaded)
logging.info("Removing old catalogs...")
_clear_preloaded(preloaded)
butler_dc2 = Butler(SOURCE_REPO, collections=SOURCE_COLLECTION)
logging.info("Transferring catalogs to data set...")
_transfer_catalogs(PRELOAD_TYPES, butler_dc2, SOURCE_COLLECTION, preloaded)
preloaded.collections.register(DEST_COLLECTION, CollectionType.CHAINED)
preloaded.collections.prepend_chain(DEST_COLLECTION, DEST_RUN)

logging.info("Preloaded APDB catalogs copied to %s:%s", DEST_DIR, DEST_COLLECTION)
